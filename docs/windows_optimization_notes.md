# Windows-Specific GPU Optimization Notes

## Multiprocessing Limitations on Windows

Windows has known issues with PyTorch's multiprocessing in DataLoader, which can cause:
- `OSError: [Errno 22] Invalid argument`
- `_pickle.UnpicklingError: pickle data was truncated`
- Training crashes when using `num_workers > 0`

## Our Solution

The optimized training script automatically detects Windows and:
1. Sets `dataloader_num_workers=0` by default
2. Uses single-process dataset preparation
3. Includes proper `multiprocessing.freeze_support()`

## Optimization Strategies for Windows

Despite the multiprocessing limitation, you can still achieve good GPU utilization through:

### 1. **Larger Batch Sizes**
Since we can't parallelize data loading, use larger batches to keep GPU busy:
```bash
--batch_size 48  # or even 64 for small model
--gradient_accumulation_steps 2
```

### 2. **Mixed Precision Training**
Use BF16 (better than FP16 on RTX 4080):
```bash
--bf16
```

### 3. **Dataset Caching**
After first run, subsequent runs will be faster due to HuggingFace dataset caching.

### 4. **Faster Audio Loading**
The script uses `soundfile` which is 2-3x faster than `librosa`.

## Performance Expectations on Windows

| Metric | Original | Windows Optimized |
|--------|----------|-------------------|
| GPU Utilization | 30-40% (very bursty) | 60-75% (more stable) |
| Data Loading | Single-threaded, slow | Single-threaded, fast |
| Training Speed | Baseline | ~2x faster |

## Alternative: WSL2

For best performance, consider using WSL2 (Windows Subsystem for Linux):
1. Full multiprocessing support
2. Better I/O performance
3. Native Linux PyTorch optimizations

To use in WSL2:
```bash
# In WSL2 terminal
python training/fine_tune_optimized.py \
    --dataloader_num_workers 4 \
    # ... other args
```

## Monitoring Tips

### Real-time GPU monitoring:
```powershell
# PowerShell
while ($true) { nvidia-smi; Start-Sleep -Seconds 1; Clear-Host }
```

### Task Manager:
- Open Task Manager â†’ Performance tab
- Select GPU to see utilization graphs

### GPU-Z:
- Download GPU-Z for detailed GPU monitoring
- Shows memory controller load, helping identify I/O bottlenecks

## Troubleshooting

### If training is still too slow:
1. Reduce batch size if OOM
2. Ensure you're using an SSD for dataset storage
3. Close other GPU-using applications
4. Check Windows Defender isn't scanning dataset files

### If GPU usage is still bursty:
1. Try even larger batch sizes
2. Ensure chunking is enabled for long audio files
3. Check disk I/O isn't the bottleneck

## Summary

While Windows has multiprocessing limitations, the optimized script still provides significant improvements through:
- Faster audio loading
- Better memory management  
- Mixed precision training
- Automatic Windows detection

For absolute best performance, consider WSL2 or dual-boot Linux. 